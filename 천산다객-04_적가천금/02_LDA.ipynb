{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 웹소설 제목 변수 설정\n",
    "webnovel_title = '천산다객-적가천금'         # 웹소설 제목 변수 설정\n",
    "\n",
    "# 설정 로깅\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "# 불용어 목록 확장\n",
    "stop_words = set([\n",
    "    '를', '이', '은', '는', '있다', '하다', '에', 'ㅠ', 'ㅋ',\n",
    "    '건가', 'ㅎ', '일이', '무슨', '대한', '슈도', '뭔가', '진짜',\n",
    "    '정말', '생각', '사람', '보고', '누구', '정도', '위해', '때문', '이건',\n",
    "    '어디', '가장', '아주', '제일', '그냥', '해도', '하나', '얼마나', '자기',\n",
    "    '부분', '어찌', '저런', '자신', '것', '수', '등', '및', '점'\n",
    "])\n",
    "\n",
    "# 단어를 치환하는 것(예를 들어 부수를 부수의로 치환해서 계산하는 것임)\n",
    "word_mapped = {\"천산\": \"천산다객\",\"산다\": \"천산다객\",\"다객\": \"천산다객\",\n",
    "               \"리가\": \"강리\",\"강원\": \"강원백\",\"은지\": \"은지여\",\"지여\": \"은지여\",\n",
    "               \"형\": \"희형\",\"강유\": \"강유요\",\"여장\": \"여장성\"}\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_excel(f'{webnovel_title}-Total_collected_data.xlsx', na_values=['NaN'])\n",
    "data = data.dropna()  # NaN 값을 가진 행 제거\n",
    "data.reset_index(drop=True, inplace=True)  # 인덱스 재설정\n",
    "\n",
    "# 텍스트 열 추출\n",
    "texts = data[\"Review\"].astype(str).tolist()\n",
    "\n",
    "# Okt 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess(text):\n",
    "    # 특수 문자와 숫자 제거\n",
    "    text = re.sub(r\"[^가-힣\\s]\", \"\", text)\n",
    "    # Okt를 사용하여 토큰화\n",
    "    tokens = okt.nouns(text)\n",
    "    # 단어 치환\n",
    "    tokens = [word_mapped.get(token, token) for token in tokens]\n",
    "    # 불용어 제거\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    return tokens\n",
    "\n",
    "# 전처리된 텍스트\n",
    "processed_texts = [preprocess(text) for text in tqdm(texts)]\n",
    "\n",
    "# 사전과 말뭉치 생성\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "# LDA 모델 설정\n",
    "num_topics = 5\n",
    "chunksize = 4000\n",
    "passes = 30\n",
    "iterations = 500\n",
    "eval_every = 10\n",
    "\n",
    "# eval_every = None\n",
    "\n",
    "# LDA 모델 학습\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    chunksize=chunksize,\n",
    "    alpha=\"auto\",\n",
    "    eta=\"auto\",\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "# 토픽 코히런스 계산\n",
    "top_topics = model.top_topics(corpus)\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print(\"Average topic coherence: %.4f.\" % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n",
    "\n",
    "# 시각화 준비\n",
    "lda_visualization = gensimvis.prepare(model, corpus, dictionary, n_jobs=1)\n",
    "pyLDAvis.save_html(lda_visualization, f\"{webnovel_title}-LDA.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
